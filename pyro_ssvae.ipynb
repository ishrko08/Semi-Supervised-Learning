{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5053b8a5-8d31-41e1-9e31-5f361ce2ef69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60f1d0af-1e57-4522-9e4d-328e902afbe9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(labels, output_size=10):\n",
    "    return torch.nn.functional.one_hot(labels, num_classes=output_size)\n",
    "\n",
    "# Binarize the MNIST dataset by thresholding at 0.5\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                               transforms.Lambda(lambda x: (x > 0.5).float().view(-1))])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "labeled_size = int(0.1 * len(train_dataset))  # use 10% of the dataset for labels\n",
    "unlabeled_size = len(train_dataset) - labeled_size\n",
    "labeled_dataset, unlabeled_dataset = random_split(train_dataset, [labeled_size, unlabeled_size])\n",
    "\n",
    "labeled_loader = DataLoader(labeled_dataset, batch_size=128, shuffle=True)\n",
    "unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44e99e66-8b62-450e-8eff-ec72bca0f05f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Encoder_z(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, latent_dim * 2)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "class Encoder_y(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_classes),\n",
    "            nn.Softmax(dim=1) \n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, num_classes, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(latent_dim + num_classes, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13181548-47ef-4e08-bb53-b47c40d12453",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SSVAE(nn.Module):\n",
    "    def __init__(self, input_size=784, hidden_size=256, z_dim=20, output_size=10):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.z_dim = z_dim\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.encoder_z = Encoder_z(input_dim=input_size + output_size,\n",
    "                                   hidden_dim=hidden_size,\n",
    "                                   latent_dim=z_dim)\n",
    "        \n",
    "        self.encoder_y = Encoder_y(input_dim=input_size,\n",
    "                                   hidden_dim=hidden_size,\n",
    "                                   num_classes=output_size)\n",
    "        \n",
    "        self.decoder = Decoder(latent_dim=z_dim,\n",
    "                               num_classes=output_size,\n",
    "                               hidden_dim=hidden_size,\n",
    "                               output_dim=input_size)\n",
    "        \n",
    "        self.optimizer = Adam({\"lr\": 0.0001})\n",
    "        self.svi = SVI(self.model, self.guide, self.optimizer, loss=Trace_ELBO())\n",
    "\n",
    "    def model(self, xs, ys=None):\n",
    "        pyro.module(\"ss_vae\", self)\n",
    "        batch_size = xs.size(0)\n",
    "        with pyro.plate(\"data\", batch_size):\n",
    "            #Prior for z \n",
    "            prior_loc = xs.new_zeros([batch_size, self.z_dim])\n",
    "            prior_scale = xs.new_ones([batch_size, self.z_dim])\n",
    "            zs = pyro.sample(\"z\", dist.Normal(prior_loc, prior_scale).to_event(1))\n",
    "            \n",
    "            #Prior for y\n",
    "            alpha_prior = xs.new_ones([batch_size, self.output_size]) / self.output_size\n",
    "            ys = pyro.sample(\"y\", dist.OneHotCategorical(alpha_prior), obs=ys)\n",
    "            \n",
    "            #Decode: concatenate z and y to get x\n",
    "            x_input = torch.cat([zs, ys], dim=1)\n",
    "            loc = self.decoder(x_input)\n",
    "            \n",
    "            pyro.sample(\"x\", dist.Bernoulli(loc).to_event(1), obs=xs)           \n",
    "\n",
    "    def guide(self, xs, ys=None):\n",
    "        pyro.module(\"ss_vae\", self)\n",
    "        with pyro.plate(\"data\", xs.size(0)):\n",
    "            if ys is None:\n",
    "                # Infer y from x using encoder_y\n",
    "                alpha = self.encoder_y(xs)\n",
    "                ys = pyro.sample(\"y\", dist.OneHotCategorical(alpha))\n",
    "            # Concatenate x and y as input to encoder_z\n",
    "            combined_input = torch.cat([xs, ys], dim=1)\n",
    "            z_params = self.encoder_z(combined_input)\n",
    "            z_loc = z_params[:, :self.z_dim]\n",
    "            z_scale = F.softplus(z_params[:, self.z_dim:]) + 1e-6\n",
    "            pyro.sample(\"z\", dist.Normal(z_loc, z_scale).to_event(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8d93876-2584-4034-b0e1-75f89a5baeef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 3105.5043\n",
      "Epoch 2, Loss: 2126.9991\n",
      "Epoch 3, Loss: 1866.8105\n",
      "Epoch 4, Loss: 1726.6974\n",
      "Epoch 5, Loss: 1617.2250\n",
      "Epoch 6, Loss: 1538.8469\n",
      "Epoch 7, Loss: 1482.2458\n",
      "Epoch 8, Loss: 1439.7116\n",
      "Epoch 9, Loss: 1405.2551\n",
      "Epoch 10, Loss: 1372.0925\n",
      "Epoch 11, Loss: 1335.7618\n",
      "Epoch 12, Loss: 1302.5601\n",
      "Epoch 13, Loss: 1275.7367\n",
      "Epoch 14, Loss: 1253.5818\n",
      "Epoch 15, Loss: 1234.1487\n",
      "Epoch 16, Loss: 1216.1454\n",
      "Epoch 17, Loss: 1200.1730\n",
      "Epoch 18, Loss: 1185.3663\n",
      "Epoch 19, Loss: 1172.8129\n",
      "Epoch 20, Loss: 1160.4880\n",
      "Epoch 21, Loss: 1148.6994\n",
      "Epoch 22, Loss: 1137.6543\n",
      "Epoch 23, Loss: 1127.5186\n",
      "Epoch 24, Loss: 1117.6941\n",
      "Epoch 25, Loss: 1108.7674\n",
      "Epoch 26, Loss: 1100.2979\n",
      "Epoch 27, Loss: 1092.1967\n",
      "Epoch 28, Loss: 1085.3353\n",
      "Epoch 29, Loss: 1078.3681\n",
      "Epoch 30, Loss: 1072.2545\n",
      "Epoch 31, Loss: 1066.5907\n",
      "Epoch 32, Loss: 1061.0929\n",
      "Epoch 33, Loss: 1056.1773\n",
      "Epoch 34, Loss: 1051.3349\n",
      "Epoch 35, Loss: 1047.2761\n",
      "Epoch 36, Loss: 1043.2709\n",
      "Epoch 37, Loss: 1038.7130\n",
      "Epoch 38, Loss: 1035.1003\n",
      "Epoch 39, Loss: 1031.8208\n",
      "Epoch 40, Loss: 1028.2445\n",
      "Epoch 41, Loss: 1024.9555\n",
      "Epoch 42, Loss: 1021.3504\n",
      "Epoch 43, Loss: 1018.8635\n",
      "Epoch 44, Loss: 1015.7543\n",
      "Epoch 45, Loss: 1012.6719\n",
      "Epoch 46, Loss: 1009.9832\n",
      "Epoch 47, Loss: 1007.1809\n",
      "Epoch 48, Loss: 1004.3900\n",
      "Epoch 49, Loss: 1001.5635\n",
      "Epoch 50, Loss: 999.3104\n",
      "Epoch 51, Loss: 997.0074\n",
      "Epoch 52, Loss: 994.7281\n",
      "Epoch 53, Loss: 992.7284\n",
      "Epoch 54, Loss: 990.4442\n",
      "Epoch 55, Loss: 988.6236\n",
      "Epoch 56, Loss: 986.4585\n",
      "Epoch 57, Loss: 984.5326\n",
      "Epoch 58, Loss: 982.6084\n",
      "Epoch 59, Loss: 980.9781\n",
      "Epoch 60, Loss: 979.2018\n",
      "Epoch 61, Loss: 977.5460\n",
      "Epoch 62, Loss: 975.9040\n",
      "Epoch 63, Loss: 974.4091\n",
      "Epoch 64, Loss: 972.9661\n",
      "Epoch 65, Loss: 971.7110\n",
      "Epoch 66, Loss: 970.2187\n",
      "Epoch 67, Loss: 968.5718\n",
      "Epoch 68, Loss: 967.3393\n",
      "Epoch 69, Loss: 966.1329\n",
      "Epoch 70, Loss: 964.9252\n",
      "Epoch 71, Loss: 963.8452\n",
      "Epoch 72, Loss: 962.7511\n",
      "Epoch 73, Loss: 961.6047\n",
      "Epoch 74, Loss: 960.5351\n",
      "Epoch 75, Loss: 959.4666\n",
      "Epoch 76, Loss: 957.9130\n",
      "Epoch 77, Loss: 956.9255\n",
      "Epoch 78, Loss: 955.8228\n",
      "Epoch 79, Loss: 955.0299\n",
      "Epoch 80, Loss: 954.0951\n",
      "Epoch 81, Loss: 953.0998\n",
      "Epoch 82, Loss: 952.1427\n",
      "Epoch 83, Loss: 951.3553\n",
      "Epoch 84, Loss: 950.3342\n",
      "Epoch 85, Loss: 949.1227\n",
      "Epoch 86, Loss: 948.3271\n",
      "Epoch 87, Loss: 947.6343\n",
      "Epoch 88, Loss: 946.8311\n",
      "Epoch 89, Loss: 945.9826\n",
      "Epoch 90, Loss: 945.2688\n",
      "Epoch 91, Loss: 944.1082\n",
      "Epoch 92, Loss: 943.5920\n",
      "Epoch 93, Loss: 942.9213\n",
      "Epoch 94, Loss: 942.0722\n",
      "Epoch 95, Loss: 941.2725\n",
      "Epoch 96, Loss: 940.3789\n",
      "Epoch 97, Loss: 939.9691\n",
      "Epoch 98, Loss: 938.9169\n",
      "Epoch 99, Loss: 938.4998\n",
      "Epoch 100, Loss: 937.7766\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "ssvae = SSVAE(input_size=784, hidden_size=128, z_dim=20, output_size=10)\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    ssvae.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    # Training on labeled data\n",
    "    for x, y in labeled_loader:\n",
    "        x = x.view(-1, 784)\n",
    "        y = one_hot_encode(y, output_size=10)\n",
    "        loss = ssvae.svi.step(x, y)\n",
    "        total_loss += loss\n",
    "\n",
    "    # Training on unlabeled data\n",
    "    for x, _ in unlabeled_loader:\n",
    "        x = x.view(-1, 784)\n",
    "        loss = ssvae.svi.step(x)\n",
    "        total_loss += loss\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(labeled_loader.dataset):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46035e11-abe5-4566-a187-26e175c4730c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "168e65de-b44a-4143-8cd8-7b312fa88c39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.view(-1))  # Flatten the 28x28 image\n",
    "])\n",
    "\n",
    "# Create the MNIST test dataset.\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "# Create a DataLoader for the test dataset.\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7c4cfc5-d37d-484e-a450-25b560916acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classification(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            # If x is in image format (e.g., [batch, 1, 28, 28]), flatten it.\n",
    "            if len(x.shape) > 2:\n",
    "                x = x.view(-1, model.input_size)\n",
    "            \n",
    "            # Get predicted class probabilities and determine predictions.\n",
    "            y_prob = model.encoder_y(x)\n",
    "            y_pred = torch.argmax(y_prob, dim=1)\n",
    "            \n",
    "            correct += (y_pred == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eea46d1b-a841-4763-be30-97a27d792d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.0903\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluate_classification(ssvae, test_loader)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518ac22c-b040-47e2-81e4-18663adfe50b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
