{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0a5baacc-9f5a-4731-8833-0ba079c9f04b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms \n",
    "from torch.utils.data import DataLoader \n",
    "import torch.nn.functional as F\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, JitTrace_ELBO, Trace_ELBO\n",
    "from pyro.optim import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745a787a-0111-4b37-86da-213be7fc977e",
   "metadata": {},
   "source": [
    "## Classical VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "382e188d-48d8-41e4-b3a1-e6d835735d62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# VAE encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, z_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, hidden_dim)\n",
    "        self.fc21 = nn.Linear(hidden_dim, z_dim)\n",
    "        self.fc22 = nn.Linear(hidden_dim, z_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden = torch.relu(self.fc1(x))\n",
    "        z_loc = self.fc21(hidden) # mean vector\n",
    "        z_scale = torch.exp(0.5*self.fc22(hidden)) # covariance vector\n",
    "        return z_loc, z_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e9e44aef-cee7-49b4-8f05-1be83cf48ed6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, z_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(z_dim, hidden_dim)\n",
    "        self.fc21 = nn.Linear(hidden_dim, 784)\n",
    "\n",
    "    def forward(self, z):\n",
    "        hidden = torch.relu(self.fc1(z))\n",
    "        loc_img = torch.sigmoid(self.fc21(hidden))\n",
    "        return loc_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "15171be0-895b-478f-ac9f-593b28771d9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, z_dim=10, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        # create the encoder and decoder networks\n",
    "        self.encoder = Encoder(z_dim, hidden_dim)\n",
    "        self.decoder = Decoder(z_dim, hidden_dim)\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "    # define the model p(x|z)p(z)\n",
    "    def model(self, x):\n",
    "        pyro.module(\"decoder\", self.decoder)\n",
    "        with pyro.plate(\"data\", x.shape[0]):\n",
    "            # prior p(z) as standard normal\n",
    "            z_loc = torch.zeros(x.shape[0], self.z_dim)\n",
    "            z_scale = torch.ones(x.shape[0], self.z_dim)\n",
    "            # sample from prior\n",
    "            z = pyro.sample(\"latent\", dist.Normal(z_loc, z_scale).to_event(1))\n",
    "            # decode the latent code z\n",
    "            loc_img = self.decoder.forward(z)\n",
    "            \n",
    "            pyro.sample(\n",
    "                \"obs\",\n",
    "                dist.Bernoulli(loc_img, validate_args=False).to_event(1),\n",
    "                obs=x.reshape(-1, 784),\n",
    "            )\n",
    "            \n",
    "            return loc_img\n",
    "\n",
    "    # define the guide (i.e. variational distribution) q(z|x)\n",
    "    def guide(self, x):\n",
    "        pyro.module(\"encoder\", self.encoder)\n",
    "        with pyro.plate(\"data\", x.shape[0]):\n",
    "            # use the encoder to get the parameters used to define q(z|x)\n",
    "            z_loc, z_scale = self.encoder.forward(x)\n",
    "            # sample the latent code z\n",
    "            pyro.sample(\"latent\", dist.Normal(z_loc, z_scale).to_event(1))\n",
    "\n",
    "    \n",
    "    def reconstruct_img(self, x):\n",
    "        # encode image x\n",
    "        z_loc, z_scale = self.encoder(x)\n",
    "        # sample in latent space\n",
    "        z = dist.Normal(z_loc, z_scale).sample()\n",
    "        # decode the image\n",
    "        loc_img = self.decoder(z)\n",
    "        return loc_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d317e8d6-0f56-4a84-bbd8-55b95aba9c3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#input_dim = 784\n",
    "lr = 0.01\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "\n",
    "train_data = datasets.MNIST(\n",
    "    root='./data', \n",
    "    train=True, \n",
    "    transform=transforms.ToTensor(), \n",
    "    download=True)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "vae = VAE()\n",
    "optimizer = Adam({\"lr\": lr})\n",
    "svi = SVI(vae.model, vae.guide, optimizer, loss=Trace_ELBO())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3b7fa988-cd73-4299-a7b1-8c284dd39c19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 546.0059057373047\n",
      "Epoch 2, Loss: 545.9868314229329\n",
      "Epoch 3, Loss: 545.990926147461\n",
      "Epoch 4, Loss: 546.0077961425782\n",
      "Epoch 5, Loss: 545.9942545776368\n",
      "Epoch 6, Loss: 546.0172881734212\n",
      "Epoch 7, Loss: 545.9797055480957\n",
      "Epoch 8, Loss: 545.9897087402344\n",
      "Epoch 9, Loss: 545.9915152526855\n",
      "Epoch 10, Loss: 545.9966286946615\n"
     ]
    }
   ],
   "source": [
    "#training \n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0\n",
    "    for x, _ in train_dataloader:\n",
    "        x = x.view(-1, 28*28)\n",
    "        train_loss += svi.step(x)\n",
    "        \n",
    "    train_loss /= len(train_dataloader.dataset)\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {train_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f53b503-1a0e-4435-bf34-48c83b4c41de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7107eb6f-3d50-4960-a5f1-09f6c049cd80",
   "metadata": {},
   "source": [
    "## SS VAE (M2 model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2745e6cc-7e7b-44e6-9a0e-0b78880c9e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(self, xs, ys=None):\n",
    "    pyro.module(\"ss_vae\", self)\n",
    "    batch_size = xs.size(0)\n",
    "\n",
    "    with pyro.plate(\"data\"):\n",
    "\n",
    "        # sample z\n",
    "        prior_loc = xs.new_zeros([batch_size, self.z_dim])\n",
    "        prior_scale = xs.new_ones([batch_size, self.z_dim])\n",
    "        zs = pyro.sample(\"z\", dist.Normal(prior_loc, prior_scale).to_event(1))\n",
    "\n",
    "        # if the label y (which digit to write) is supervised, sample from the\n",
    "        # constant prior, otherwise, observe the value (i.e. score it against the constant prior)\n",
    "        alpha_prior = xs.new_ones([batch_size, self.output_size]) / (1.0 * self.output_size)\n",
    "        ys = pyro.sample(\"y\", dist.OneHotCategorical(alpha_prior), obs=ys)\n",
    "\n",
    "        # finally, score the image (x) using the handwriting style (z) and\n",
    "        # the class label y (which digit to write) against the\n",
    "        # parametrized distribution p(x|y,z) = bernoulli(decoder(y,z))\n",
    "        # where `decoder` is a neural network\n",
    "        loc = self.decoder([zs, ys])\n",
    "        pyro.sample(\"x\", dist.Bernoulli(loc).to_event(1), obs=xs)\n",
    "\n",
    "def guide(self, xs, ys=None):\n",
    "    with pyro.plate(\"data\"):\n",
    "        # if the class label (the digit) is not supervised, sample\n",
    "        # (and score) the digit with the variational distribution\n",
    "        # q(y|x) = categorical(alpha(x))\n",
    "        if ys is None:\n",
    "            alpha = self.encoder_y(xs)\n",
    "            ys = pyro.sample(\"y\", dist.OneHotCategorical(alpha))\n",
    "\n",
    "        # sample (and score) the latent handwriting-style with the variational\n",
    "        # distribution q(z|x,y) = normal(loc(x,y),scale(x,y))\n",
    "        loc, scale = self.encoder_z([xs, ys])\n",
    "        pyro.sample(\"z\", dist.Normal(loc, scale).to_event(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1585e078-8573-41a4-a771-b6fadd8ff230",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b21f1157-82e6-4a4b-80fd-1d6e465cd509",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000],\n",
       "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000],\n",
       "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000],\n",
       "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000],\n",
       "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000],\n",
       "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000],\n",
       "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000],\n",
       "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000],\n",
       "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000],\n",
       "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000],\n",
       "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000],\n",
       "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000],\n",
       "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000],\n",
       "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000],\n",
       "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000],\n",
       "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xxx = torch.tensor((), dtype=torch.int32)\n",
    "xxx.new_ones([16, 10]) / (1.0 * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7975285f-c232-4220-b947-ec3fdc1b03c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea37452-dd67-432c-973f-0a1318fa2d0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bbf54b-42c4-4712-8628-4ff9a668f9f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
