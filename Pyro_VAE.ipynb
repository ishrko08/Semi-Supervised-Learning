{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0a5baacc-9f5a-4731-8833-0ba079c9f04b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms \n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, JitTrace_ELBO, Trace_ELBO\n",
    "from pyro.optim import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745a787a-0111-4b37-86da-213be7fc977e",
   "metadata": {},
   "source": [
    "## Classical VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "382e188d-48d8-41e4-b3a1-e6d835735d62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# VAE encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, z_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, hidden_dim)\n",
    "        self.fc21 = nn.Linear(hidden_dim, z_dim)\n",
    "        self.fc22 = nn.Linear(hidden_dim, z_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden = torch.relu(self.fc1(x))\n",
    "        z_loc = self.fc21(hidden) # mean vector\n",
    "        z_scale = torch.exp(0.5*self.fc22(hidden)) # covariance vector\n",
    "        return z_loc, z_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e9e44aef-cee7-49b4-8f05-1be83cf48ed6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, z_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(z_dim, hidden_dim)\n",
    "        self.fc21 = nn.Linear(hidden_dim, 784)\n",
    "\n",
    "    def forward(self, z):\n",
    "        hidden = torch.relu(self.fc1(z))\n",
    "        loc_img = torch.sigmoid(self.fc21(hidden))\n",
    "        return loc_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "15171be0-895b-478f-ac9f-593b28771d9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, z_dim=10, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        # create the encoder and decoder networks\n",
    "        self.encoder = Encoder(z_dim, hidden_dim)\n",
    "        self.decoder = Decoder(z_dim, hidden_dim)\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "    # define the model p(x|z)p(z)\n",
    "    def model(self, x):\n",
    "        pyro.module(\"decoder\", self.decoder)\n",
    "        with pyro.plate(\"data\", x.shape[0]):\n",
    "            # prior p(z) as standard normal\n",
    "            z_loc = torch.zeros(x.shape[0], self.z_dim)\n",
    "            z_scale = torch.ones(x.shape[0], self.z_dim)\n",
    "            # sample from prior\n",
    "            z = pyro.sample(\"latent\", dist.Normal(z_loc, z_scale).to_event(1))\n",
    "            # decode the latent code z\n",
    "            loc_img = self.decoder.forward(z)\n",
    "            \n",
    "            pyro.sample(\n",
    "                \"obs\",\n",
    "                dist.Bernoulli(loc_img, validate_args=False).to_event(1),\n",
    "                obs=x.reshape(-1, 784),\n",
    "            )\n",
    "            \n",
    "            return loc_img\n",
    "\n",
    "    # define the guide (i.e. variational distribution) q(z|x)\n",
    "    def guide(self, x):\n",
    "        pyro.module(\"encoder\", self.encoder)\n",
    "        with pyro.plate(\"data\", x.shape[0]):\n",
    "            # use the encoder to get the parameters used to define q(z|x)\n",
    "            z_loc, z_scale = self.encoder.forward(x)\n",
    "            # sample the latent code z\n",
    "            pyro.sample(\"latent\", dist.Normal(z_loc, z_scale).to_event(1))\n",
    "\n",
    "    \n",
    "    def reconstruct_img(self, x):\n",
    "        # encode image x\n",
    "        z_loc, z_scale = self.encoder(x)\n",
    "        # sample in latent space\n",
    "        z = dist.Normal(z_loc, z_scale).sample()\n",
    "        # decode the image\n",
    "        loc_img = self.decoder(z)\n",
    "        return loc_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d317e8d6-0f56-4a84-bbd8-55b95aba9c3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#input_dim = 784\n",
    "lr = 0.01\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "\n",
    "train_data = datasets.MNIST(\n",
    "    root='./data', \n",
    "    train=True, \n",
    "    transform=transforms.ToTensor(), \n",
    "    download=True)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "vae = VAE()\n",
    "optimizer = Adam({\"lr\": lr})\n",
    "svi = SVI(vae.model, vae.guide, optimizer, loss=Trace_ELBO())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3b7fa988-cd73-4299-a7b1-8c284dd39c19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 546.0059057373047\n",
      "Epoch 2, Loss: 545.9868314229329\n",
      "Epoch 3, Loss: 545.990926147461\n",
      "Epoch 4, Loss: 546.0077961425782\n",
      "Epoch 5, Loss: 545.9942545776368\n",
      "Epoch 6, Loss: 546.0172881734212\n",
      "Epoch 7, Loss: 545.9797055480957\n",
      "Epoch 8, Loss: 545.9897087402344\n",
      "Epoch 9, Loss: 545.9915152526855\n",
      "Epoch 10, Loss: 545.9966286946615\n"
     ]
    }
   ],
   "source": [
    "#training \n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0\n",
    "    for x, _ in train_dataloader:\n",
    "        x = x.view(-1, 28*28)\n",
    "        train_loss += svi.step(x)\n",
    "        \n",
    "    train_loss /= len(train_dataloader.dataset)\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {train_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f53b503-1a0e-4435-bf34-48c83b4c41de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7107eb6f-3d50-4960-a5f1-09f6c049cd80",
   "metadata": {},
   "source": [
    "## SS VAE (M2 model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6265f8-f463-41f7-b76a-23ec6402d279",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610e5e72-e64a-499d-87a3-b549bb8fa852",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "33fef0d2-d07b-42ef-a14d-5438119814ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# VAE encoder for z\n",
    "class Encoder_Z(nn.Module):\n",
    "    def __init__(self, z_dim=10, input_dim=784, hidden_dim=128, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim+num_classes, hidden_dim)\n",
    "        self.fc21 = nn.Linear(hidden_dim, z_dim)\n",
    "        self.fc22 = nn.Linear(hidden_dim, z_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden = torch.relu(self.fc1(x))\n",
    "        z_loc = self.fc21(hidden) # mean vector\n",
    "        z_scale = torch.exp(0.5*self.fc22(hidden)) # covariance vector\n",
    "        return z_loc, z_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "7cb2f328-8b75-4d2f-9d0e-20381aabe810",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# VAE encoder for y\n",
    "class Encoder_Y(nn.Module):\n",
    "    def __init__(self, input_dim=784, hidden_dim=128, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc21 = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden = torch.relu(self.fc1(x))\n",
    "        y_alpha = torch.softmax(self.fc21(hidden), dim=1)\n",
    "        return y_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "f165d71a-f1f5-408c-aa9d-65efb44659e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, z_dim=10, input_dim=784, hidden_dim=128, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(z_dim + num_classes, hidden_dim)\n",
    "        self.fc21 = nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "    def forward(self, z):\n",
    "        hidden = torch.relu(self.fc1(z))\n",
    "        loc_img = torch.sigmoid(self.fc21(hidden))\n",
    "        return loc_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "1585e078-8573-41a4-a771-b6fadd8ff230",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SSVAE(nn.Module):\n",
    "    def __init__(self, z_dim=10, hidden_dim=128, num_classes=10):\n",
    "        super().__init__()\n",
    "        # Encoders and decoder\n",
    "        self.z_dim = z_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.output_size = num_classes\n",
    "        self.encoder_y = Encoder_Y()\n",
    "        self.encoder_z = Encoder_Z()\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "    def model(self, xs, ys=None):\n",
    "        pyro.module(\"ss_vae\", self)\n",
    "        batch_size = xs.size(0)\n",
    "\n",
    "        with pyro.plate(\"data\"):\n",
    "            # Sample latent variable z\n",
    "            prior_loc = xs.new_zeros([batch_size, self.z_dim])\n",
    "            prior_scale = xs.new_ones([batch_size, self.z_dim])\n",
    "            zs = pyro.sample(\"z\", dist.Normal(prior_loc, prior_scale).to_event(1))\n",
    "\n",
    "            # Sample labels y (if unobserved)\n",
    "            alpha_prior = xs.new_ones([batch_size, self.output_size]) / self.output_size\n",
    "            ys = pyro.sample(\"y\", dist.OneHotCategorical(alpha_prior), obs=ys)\n",
    "\n",
    "            # Decode (concatenate z and y)\n",
    "            combined_input = torch.cat([zs, ys], dim=1)\n",
    "            loc = self.decoder(combined_input)\n",
    "\n",
    "            # Sample observations x\n",
    "            pyro.sample(\"x\", dist.Bernoulli(loc).to_event(1), obs=xs)\n",
    "\n",
    "    def guide(self, xs, ys=None):\n",
    "        with pyro.plate(\"data\"):\n",
    "            # If y is not observed, sample it\n",
    "            if ys is None:\n",
    "                alpha = self.encoder_y(xs)\n",
    "                ys = pyro.sample(\"y\", dist.OneHotCategorical(alpha))\n",
    "\n",
    "            # Sample latent variable z\n",
    "            combined_input = torch.cat([xs, ys], dim=1)\n",
    "            loc, scale = self.encoder_z(combined_input)\n",
    "            pyro.sample(\"z\", dist.Normal(loc, scale).to_event(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "7975285f-c232-4220-b947-ec3fdc1b03c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Transform: Flatten images and normalize to [0, 1]\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: torch.flatten(x)),\n",
    "])\n",
    "\n",
    "mnist_train = datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
    "mnist_test = datasets.MNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
    "\n",
    "# Split train data into labeled and unlabeled subsets\n",
    "num_labeled = 1000  # Number of labeled samples\n",
    "num_unlabeled = len(mnist_train) - num_labeled\n",
    "labeled_data, unlabeled_data = random_split(mnist_train, [num_labeled, num_unlabeled])\n",
    "\n",
    "batch_size = 128\n",
    "labeled_loader = DataLoader(labeled_data, batch_size=batch_size, shuffle=True)\n",
    "unlabeled_loader = DataLoader(unlabeled_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "6ea37452-dd67-432c-973f-0a1318fa2d0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(labels, num_classes=10):\n",
    "    return torch.eye(num_classes)[labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "72bbf54b-42c4-4712-8628-4ff9a668f9f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error while computing log_prob at site 'x':\nExpected value argument (Tensor of shape (128, 784)) to be within the support (Boolean()) of the distribution Bernoulli(probs: torch.Size([128, 784])), but found invalid values:\ntensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.]])\n                 Trace Shapes:            \n                  Param Sites:            \n ss_vae$$$encoder_y.fc1.weight 128 784    \n   ss_vae$$$encoder_y.fc1.bias     128    \nss_vae$$$encoder_y.fc21.weight  10 128    \n  ss_vae$$$encoder_y.fc21.bias      10    \n ss_vae$$$encoder_z.fc1.weight 128 794    \n   ss_vae$$$encoder_z.fc1.bias     128    \nss_vae$$$encoder_z.fc21.weight  10 128    \n  ss_vae$$$encoder_z.fc21.bias      10    \nss_vae$$$encoder_z.fc22.weight  10 128    \n  ss_vae$$$encoder_z.fc22.bias      10    \n   ss_vae$$$decoder.fc1.weight 128  20    \n     ss_vae$$$decoder.fc1.bias     128    \n  ss_vae$$$decoder.fc21.weight 784 128    \n    ss_vae$$$decoder.fc21.bias     784    \n                 Sample Sites:            \n                        z dist 128   |  10\n                         value 128   |  10\n                      log_prob 128   |    \n                        y dist 128   |  10\n                         value 128   |  10\n                      log_prob 128   |    \n                        x dist 128   | 784\n                         value 128   | 784",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pyro\\poutine\\trace_struct.py:264\u001b[0m, in \u001b[0;36mTrace.compute_log_prob\u001b[1;34m(self, site_filter)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 264\u001b[0m     log_p \u001b[38;5;241m=\u001b[39m site[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfn\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mlog_prob(\n\u001b[0;32m    265\u001b[0m         site[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;241m*\u001b[39msite[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msite[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    266\u001b[0m     )\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\distributions\\independent.py:107\u001b[0m, in \u001b[0;36mIndependent.log_prob\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlog_prob\u001b[39m(\u001b[38;5;28mself\u001b[39m, value):\n\u001b[1;32m--> 107\u001b[0m     log_prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_dist\u001b[38;5;241m.\u001b[39mlog_prob(value)\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _sum_rightmost(log_prob, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreinterpreted_batch_ndims)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\distributions\\bernoulli.py:109\u001b[0m, in \u001b[0;36mBernoulli.log_prob\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_args:\n\u001b[1;32m--> 109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_sample(value)\n\u001b[0;32m    110\u001b[0m logits, value \u001b[38;5;241m=\u001b[39m broadcast_all(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogits, value)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\distributions\\distribution.py:312\u001b[0m, in \u001b[0;36mDistribution._validate_sample\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid\u001b[38;5;241m.\u001b[39mall():\n\u001b[1;32m--> 312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    313\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected value argument \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    314\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    315\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto be within the support (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(support)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    316\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof the distribution \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    317\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    318\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Expected value argument (Tensor of shape (128, 784)) to be within the support (Boolean()) of the distribution Bernoulli(probs: torch.Size([128, 784])), but found invalid values:\ntensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.]])",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[185], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)  \n\u001b[0;32m     19\u001b[0m     y \u001b[38;5;241m=\u001b[39m one_hot_encode(y)\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)  \n\u001b[1;32m---> 20\u001b[0m     loss \u001b[38;5;241m=\u001b[39m svi\u001b[38;5;241m.\u001b[39mstep(x, y)  \n\u001b[0;32m     21\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Train on unlabeled data\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pyro\\infer\\svi.py:145\u001b[0m, in \u001b[0;36mSVI.step\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# get loss and compute gradients\u001b[39;00m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m poutine\u001b[38;5;241m.\u001b[39mtrace(param_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m param_capture:\n\u001b[1;32m--> 145\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_and_grads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mguide, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    147\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\n\u001b[0;32m    148\u001b[0m     site[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39munconstrained() \u001b[38;5;28;01mfor\u001b[39;00m site \u001b[38;5;129;01min\u001b[39;00m param_capture\u001b[38;5;241m.\u001b[39mtrace\u001b[38;5;241m.\u001b[39mnodes\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[0;32m    149\u001b[0m )\n\u001b[0;32m    151\u001b[0m \u001b[38;5;66;03m# actually perform gradient steps\u001b[39;00m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;66;03m# torch.optim objects gets instantiated for any params that haven't been seen yet\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pyro\\infer\\trace_elbo.py:140\u001b[0m, in \u001b[0;36mTrace_ELBO.loss_and_grads\u001b[1;34m(self, model, guide, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;66;03m# grab a trace from the generator\u001b[39;00m\n\u001b[1;32m--> 140\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_trace, guide_trace \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_traces(model, guide, args, kwargs):\n\u001b[0;32m    141\u001b[0m     loss_particle, surrogate_loss_particle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_differentiable_loss_particle(\n\u001b[0;32m    142\u001b[0m         model_trace, guide_trace\n\u001b[0;32m    143\u001b[0m     )\n\u001b[0;32m    144\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_particle \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_particles\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pyro\\infer\\elbo.py:237\u001b[0m, in \u001b[0;36mELBO._get_traces\u001b[1;34m(self, model, guide, args, kwargs)\u001b[0m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    236\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_particles):\n\u001b[1;32m--> 237\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_trace(model, guide, args, kwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pyro\\infer\\trace_elbo.py:57\u001b[0m, in \u001b[0;36mTrace_ELBO._get_trace\u001b[1;34m(self, model, guide, args, kwargs)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, guide, args, kwargs):\n\u001b[0;32m     53\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;124;03m    Returns a single trace from the guide, and the model that is run\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;124;03m    against it.\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     model_trace, guide_trace \u001b[38;5;241m=\u001b[39m get_importance_trace(\n\u001b[0;32m     58\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflat\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_plate_nesting, model, guide, args, kwargs\n\u001b[0;32m     59\u001b[0m     )\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_validation_enabled():\n\u001b[0;32m     61\u001b[0m         check_if_enumerated(guide_trace)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pyro\\infer\\enum.py:75\u001b[0m, in \u001b[0;36mget_importance_trace\u001b[1;34m(graph_type, max_plate_nesting, model, guide, args, kwargs, detach)\u001b[0m\n\u001b[0;32m     72\u001b[0m guide_trace \u001b[38;5;241m=\u001b[39m prune_subsample_sites(guide_trace)\n\u001b[0;32m     73\u001b[0m model_trace \u001b[38;5;241m=\u001b[39m prune_subsample_sites(model_trace)\n\u001b[1;32m---> 75\u001b[0m model_trace\u001b[38;5;241m.\u001b[39mcompute_log_prob()\n\u001b[0;32m     76\u001b[0m guide_trace\u001b[38;5;241m.\u001b[39mcompute_score_parts()\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_validation_enabled():\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pyro\\poutine\\trace_struct.py:270\u001b[0m, in \u001b[0;36mTrace.compute_log_prob\u001b[1;34m(self, site_filter)\u001b[0m\n\u001b[0;32m    268\u001b[0m     _, exc_value, traceback \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mexc_info()\n\u001b[0;32m    269\u001b[0m     shapes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_shapes(last_site\u001b[38;5;241m=\u001b[39msite[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m--> 270\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    271\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while computing log_prob at site \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    272\u001b[0m             name, exc_value, shapes\n\u001b[0;32m    273\u001b[0m         )\n\u001b[0;32m    274\u001b[0m     )\u001b[38;5;241m.\u001b[39mwith_traceback(traceback) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    275\u001b[0m site[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munscaled_log_prob\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m log_p\n\u001b[0;32m    276\u001b[0m log_p \u001b[38;5;241m=\u001b[39m scale_and_mask(log_p, site[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale\u001b[39m\u001b[38;5;124m\"\u001b[39m], site[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pyro\\poutine\\trace_struct.py:264\u001b[0m, in \u001b[0;36mTrace.compute_log_prob\u001b[1;34m(self, site_filter)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog_prob\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m site:\n\u001b[0;32m    263\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 264\u001b[0m         log_p \u001b[38;5;241m=\u001b[39m site[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfn\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mlog_prob(\n\u001b[0;32m    265\u001b[0m             site[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;241m*\u001b[39msite[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msite[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    266\u001b[0m         )\n\u001b[0;32m    267\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    268\u001b[0m         _, exc_value, traceback \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mexc_info()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\distributions\\independent.py:107\u001b[0m, in \u001b[0;36mIndependent.log_prob\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlog_prob\u001b[39m(\u001b[38;5;28mself\u001b[39m, value):\n\u001b[1;32m--> 107\u001b[0m     log_prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_dist\u001b[38;5;241m.\u001b[39mlog_prob(value)\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _sum_rightmost(log_prob, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreinterpreted_batch_ndims)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\distributions\\bernoulli.py:109\u001b[0m, in \u001b[0;36mBernoulli.log_prob\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlog_prob\u001b[39m(\u001b[38;5;28mself\u001b[39m, value):\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_args:\n\u001b[1;32m--> 109\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_sample(value)\n\u001b[0;32m    110\u001b[0m     logits, value \u001b[38;5;241m=\u001b[39m broadcast_all(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogits, value)\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39mbinary_cross_entropy_with_logits(logits, value, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\distributions\\distribution.py:312\u001b[0m, in \u001b[0;36mDistribution._validate_sample\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    310\u001b[0m valid \u001b[38;5;241m=\u001b[39m support\u001b[38;5;241m.\u001b[39mcheck(value)\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid\u001b[38;5;241m.\u001b[39mall():\n\u001b[1;32m--> 312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    313\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected value argument \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    314\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    315\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto be within the support (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(support)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    316\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof the distribution \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    317\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    318\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Error while computing log_prob at site 'x':\nExpected value argument (Tensor of shape (128, 784)) to be within the support (Boolean()) of the distribution Bernoulli(probs: torch.Size([128, 784])), but found invalid values:\ntensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.]])\n                 Trace Shapes:            \n                  Param Sites:            \n ss_vae$$$encoder_y.fc1.weight 128 784    \n   ss_vae$$$encoder_y.fc1.bias     128    \nss_vae$$$encoder_y.fc21.weight  10 128    \n  ss_vae$$$encoder_y.fc21.bias      10    \n ss_vae$$$encoder_z.fc1.weight 128 794    \n   ss_vae$$$encoder_z.fc1.bias     128    \nss_vae$$$encoder_z.fc21.weight  10 128    \n  ss_vae$$$encoder_z.fc21.bias      10    \nss_vae$$$encoder_z.fc22.weight  10 128    \n  ss_vae$$$encoder_z.fc22.bias      10    \n   ss_vae$$$decoder.fc1.weight 128  20    \n     ss_vae$$$decoder.fc1.bias     128    \n  ss_vae$$$decoder.fc21.weight 784 128    \n    ss_vae$$$decoder.fc21.bias     784    \n                 Sample Sites:            \n                        z dist 128   |  10\n                         value 128   |  10\n                      log_prob 128   |    \n                        y dist 128   |  10\n                         value 128   |  10\n                      log_prob 128   |    \n                        x dist 128   | 784\n                         value 128   | 784"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "z_dim = 10\n",
    "hidden_dim = 128\n",
    "ssvae = SSVAE(z_dim=z_dim, hidden_dim=hidden_dim)\n",
    "\n",
    "optimizer = optim.Adam({\"lr\": 0.01})\n",
    "\n",
    "svi = SVI(ssvae.model, ssvae.guide, optimizer, loss=Trace_ELBO())\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    ssvae.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    # Train on labeled data\n",
    "    for x, y in labeled_loader:\n",
    "        x = x.to(dtype=torch.float32)  \n",
    "        y = one_hot_encode(y).to(dtype=torch.float32)  \n",
    "        loss = svi.step(x, y)  \n",
    "        total_loss += loss\n",
    "\n",
    "    # Train on unlabeled data\n",
    "    for x, _ in unlabeled_loader:\n",
    "        x = x.to(dtype=torch.float32)\n",
    "        loss = svi.step(x)  \n",
    "        total_loss += loss\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(labeled_loader.dataset):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162ae9fc-afbd-44cc-baf1-4a347387dc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ssvae.eval()\n",
    "test_iter = iter(test_loader)\n",
    "x, y = next(test_iter)\n",
    "x = x.to(dtype=torch.float32)\n",
    "\n",
    "# Reconstruct images\n",
    "with torch.no_grad():\n",
    "    loc_img = ssvae.decoder(torch.cat([ssvae.encoder_y(x), x], dim=1))\n",
    "\n",
    "def visualize_reconstruction(original, reconstructed):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    for i in range(8):\n",
    "        # Original images\n",
    "        plt.subplot(2, 8, i + 1)\n",
    "        plt.imshow(original[i].view(28, 28), cmap=\"gray\")\n",
    "        plt.axis(\"off\")\n",
    "        # Reconstructed images\n",
    "        plt.subplot(2, 8, i + 9)\n",
    "        plt.imshow(reconstructed[i].view(28, 28), cmap=\"gray\")\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "visualize_reconstruction(x[:8], loc_img[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16df236c-8be0-480c-acc6-e3e51e11c553",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        x = x.to(dtype=torch.float32)\n",
    "        y_onehot = ssvae.encoder_y(x)\n",
    "        predicted = torch.argmax(y_onehot, dim=1)\n",
    "        correct += (predicted == y).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
